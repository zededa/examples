# Edge AI ML Inference Platform

A complete, production-ready machine learning inference platform with dynamic model management on Kubernetes.

## ğŸš€ Quick Start

### Deploy with Docker Hub (Recommended)
```bash
# Build any local changes
./build.sh

# Deploy using pre-built Docker Hub images
cd helm-chart/edge-ai-car-classification
helm dependency update
helm install edge-ai-car-classification . --namespace edge-ai-car-classification --create-namespace \
  --set server.ovms.image=adithyazededa/edgeai-ovms-server:latest \
  --set server.sidecar.image=adithyazededa/model-sync-sidecar:latest \
  --set client.image=adithyazededa/edgeai-client-app:latest
```

### Deploy with Local Build
```bash
# Build all images locally
./build.sh

# Deploy with locally built images
cd helm-chart/edge-ai-car-classification
helm dependency update
helm install ml-platform . --namespace ml-platform --create-namespace
```

### Cleanup
```bash
./cleanup.sh
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                       â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚   MinIO Pod     â”‚    â”‚   Server Pod    â”‚                â”‚
â”‚  â”‚                 â”‚    â”‚                 â”‚                â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                â”‚
â”‚  â”‚  â”‚   MinIO   â”‚  â”‚    â”‚ â”‚    OVMS     â”‚ â”‚                â”‚
â”‚  â”‚  â”‚  Storage  â”‚  â”‚â—„â”€â”€â”€â”¤ â”‚  Container  â”‚ â”‚                â”‚
â”‚  â”‚  â”‚           â”‚  â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚                 â”‚                â”‚
â”‚  â”‚                 â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚    Sync     â”‚ â”‚                â”‚
â”‚                         â”‚ â”‚   Sidecar   â”‚ â”‚                â”‚
â”‚                         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  â”‚   Client Pod    â”‚                                       â”‚
â”‚  â”‚                 â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ Shared Storage  â”‚                â”‚
â”‚  â”‚ â”‚   Client    â”‚ â”‚    â”‚     (PVC)       â”‚                â”‚
â”‚  â”‚ â”‚Application  â”‚ â”‚â—„â”€â”€â”€â”¤   /models/      â”‚                â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚                 â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

- **Dynamic Model Loading** - Upload models to MinIO, they're automatically loaded
- **Multi-Model Support** - Client discovers and uses all available models
- **Zero Downtime** - Add new models without restarting services
- **Production Ready** - Complete Kubernetes deployment with monitoring
- **Auto-scaling** - Horizontal pod autoscaling based on demand
- **Security** - RBAC integration and configurable secrets management

## ğŸ“ Project Structure

```
edge-ai-car-classification/
â”œâ”€â”€ client-container/          # Multi-model client application
â”œâ”€â”€ server-container/          # OpenVINO Model Server setup
â”œâ”€â”€ volume-for-model/          # Model files
â”œâ”€â”€ sync-sidecar/              # Dynamic model loading component
â”œâ”€â”€ helm-chart/                # Kubernetes deployment charts
â”œâ”€â”€ jupyter-notebook/          # ML model tracking notebook
â”œâ”€â”€ build.sh                   # Build automation script
â”œâ”€â”€ cleanup.sh                 # Cleanup automation script
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Prerequisites

- Kubernetes cluster (minikube, kind, or any K8s cluster)
- kubectl configured to access your cluster
- Helm 3.x installed
- Docker for building images

## ğŸ“¦ Deployment Options

### Option 1: Docker Hub Deployment (Easiest)
Uses pre-built images from Docker Hub - no building required:

```bash
cd helm-chart/edge-ai-car-classification
helm dependency update
helm install ml-platform . --namespace ml-platform --create-namespace \
  --set server.ovms.image=adithyazededa/edgeai-ovms-server:latest \
  --set server.sidecar.image=adithyazededa/model-sync-sidecar:latest \
  --set client.image=adithyazededa/edgeai-client-app:latest
```

### Option 2: Local Development Deployment
Builds images locally and deploys:

```bash
# Build all container images
./build.sh

# Deploy the platform with local images
cd helm-chart/edge-ai-car-classification
helm dependency update
helm install ml-platform . --namespace ml-platform --create-namespace
```

### Option 3: Custom Registry Deployment
Build and push to your own registry:

```bash
# Build and push to custom registry
REGISTRY=your-username PUSH=true ./build.sh

# Deploy with custom images
cd helm-chart/edge-ai-car-classification
helm dependency update
helm install ml-platform . --namespace ml-platform --create-namespace \
  --set server.ovms.image=your-username/edgeai-ovms-server:latest \
  --set server.sidecar.image=your-username/model-sync-sidecar:latest \
  --set client.image=your-username/edgeai-client-app:latest
```

## ğŸ”„ How Dynamic Model Loading Works

1. **Upload Model**: Add new model to MinIO bucket with correct structure:
   ```
   ml-models/
   â””â”€â”€ model_name/          # e.g., efb3_car_onnx
       â””â”€â”€ version/         # e.g., 1, 2, latest  
           â””â”€â”€ model_files  # e.g., model.onnx
   ```

2. **Sync Detection**: Sidecar detects new model via polling (every 30s)
3. **Download**: Sidecar downloads model to shared volume at `/models/`
4. **Config Update**: Sidecar updates `/models/config.json` with new model
5. **OVMS Reload**: OVMS automatically detects config change and loads model
6. **Ready**: New model is available for inference at `/v1/models/model_name:predict`

## ğŸ“Š Monitoring and Management

### Check Deployment Status
```bash
# Check all pods status
kubectl get pods -n ml-platform

# Check services and endpoints
kubectl get svc,endpoints -n ml-platform
```

### View Logs
```bash
# View OVMS Server logs
kubectl logs -l app.kubernetes.io/component=server -c ovms -n ml-platform -f

# View Model Sync Sidecar logs  
kubectl logs -l app.kubernetes.io/component=server -c model-sync -n ml-platform -f

# View Client Application logs
kubectl logs -l app.kubernetes.io/component=client -n ml-platform -f
```

### Upload Models
```bash
# Access MinIO console
kubectl port-forward svc/ml-platform-minio-console 9001:9001 -n ml-platform

# Upload models via web UI at http://localhost:9001
# Login with: minioadmin / minioadmin123
```

### Scale Components
```bash
# Scale server pods
helm upgrade ml-platform ./helm-chart/edge-ai-car-classification --set server.replicaCount=3 --reuse-values -n ml-platform

# Scale client pods  
helm upgrade ml-platform ./helm-chart/edge-ai-car-classification --set client.replicaCount=2 --reuse-values -n ml-platform
```

## ğŸ”’ Security Considerations

1. **Change default MinIO credentials** in production
2. **Use image pull secrets** for private registries
3. **Enable RBAC** with minimal permissions
4. **Use security contexts** to run as non-root
5. **Enable network policies** to restrict traffic

## ğŸ†˜ Troubleshooting

### Common Issues

#### Models not loading
- Check sync sidecar logs: `kubectl logs -l app.kubernetes.io/component=server -c model-sync -n ml-platform`
- Verify MinIO connectivity and bucket exists
- Ensure correct model directory structure

#### Client connection errors
- Verify service names and ports
- Check OVMS server is running: `kubectl get pods -l app.kubernetes.io/component=server -n ml-platform`

#### Image pull failures
- Check registry credentials and image names
- For minikube: ensure images are loaded with `minikube image load`

#### Storage issues
- Verify PVC creation: `kubectl get pvc -n ml-platform`
- Check storage class availability: `kubectl get storageclass`

### Debug Commands
```bash
# Check all platform resources
kubectl get all -l app.kubernetes.io/instance=ml-platform -n ml-platform

# Get detailed resource information
kubectl describe all -l app.kubernetes.io/instance=ml-platform -n ml-platform

# Check pod events
kubectl get events --sort-by=.metadata.creationTimestamp -n ml-platform

# Access container shell
kubectl exec -it deployment/ml-platform-edge-ai-car-classification-server -c ovms -n ml-platform -- /bin/bash
```

### Missing MinIO Bucket
```bash
# Create bucket manually using MinIO client
kubectl port-forward svc/ml-platform-minio 9000:9000 -n ml-platform
mc alias set local http://localhost:9000 minioadmin minioadmin123
mc mb local/ml-models
```

## ğŸ¯ Production Deployment

For production environments:

1. Use the production values template:
   ```bash
   cp helm-chart/edge-ai-car-classification/examples/values-prod.yaml production-values.yaml
   # Edit with production settings
   helm install ml-platform ./helm-chart/edge-ai-car-classification -f production-values.yaml
   ```

2. Key production settings:
   - Multiple replicas for high availability
   - Resource limits and requests
   - Ingress configuration
   - Auto-scaling enabled
   - Monitoring integration
   - Security hardening

## ğŸ“š Additional Documentation

- **[Client Container](client-container/README.md)** - Multi-model client application details
- **[Server Container](server-container/README.md)** - OpenVINO Model Server setup
- **[Sync Sidecar](sync-sidecar/README.md)** - Dynamic model loading component
- **[Helm Chart](helm-chart/edge-ai-car-classification/README.md)** - Kubernetes deployment details

## ğŸ‰ What You've Achieved

You now have a **complete, production-ready ML inference platform** that delivers:

âœ… **Dynamic Model Management**: Models are automatically detected, downloaded, and loaded when uploaded to MinIO storage  
âœ… **Zero-Downtime Operations**: Add new models without restarting inference services  
âœ… **Container Orchestration**: Full Kubernetes-native deployment with Helm chart management  
âœ… **Multi-Model Support**: Platform automatically detects and serves multiple ML models simultaneously  
âœ… **Scalable Infrastructure**: Horizontal pod autoscaling for inference servers based on demand  
âœ… **High Availability**: Multi-replica deployments with load balancing and health checks  
âœ… **Fully Automated Deployment**: One-command deployment with `./deploy.sh` including all dependencies  
âœ… **Production Readiness**: Security best practices, monitoring, and comprehensive documentation

## ğŸ“ Support

For issues or questions:
1. Check the troubleshooting section above
2. Review logs using the provided kubectl commands
3. Consult the detailed documentation in component README files

---

**Ready to deploy?** Start with `./build.sh` followed by the Helm deployment commands above for the quickest setup!

## ğŸ“¦ Docker Images (Docker Hub)

- `adithyazededa/edgeai-ovms-server:latest`
- `adithyazededa/edgeai-model-sync-sidecar:latest` 
- `adithyazededa/edgeai-client-app:latest`

Deploy anywhere with: `./deploy.sh --dockerhub`
